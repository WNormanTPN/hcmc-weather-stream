

services:

  ### ─── AIRFLOW ─────────────────────────────────────
  postgres15:
    image: postgres:15
    container_name: postgres15
    hostname: ${POSTGRES_HOST}
    ports:
      - "${POSTGRES_PORT}:${POSTGRES_PORT}"
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "${POSTGRES_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5


  airflow-init:
    image: apache/airflow:latest-python3.12
    container_name: airflow-init
    hostname: airflow-init
    depends_on:
      postgres15:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:
        postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
    command: >
      bash -c "airflow db init &&
      airflow users create --username admin --firstname Admin --lastname User
      --role Admin --email admin@example.com
      --password admin || echo 'Admin user already exists'"
    healthcheck:
      test: ["CMD-SHELL", "airflow db check || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3


  airflow-webserver:
    image: apache/airflow:latest-python3.12
    container_name: airflow-webserver
    hostname: airflow-webserver
    depends_on:
      postgres15:
        condition: service_healthy
      airflow-init:
        condition: service_completed_successfully
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:
        postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Ho_Chi_Minh
      TZ: Asia/Ho_Chi_Minh
    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/requirements.in:/requirements.in
      - ./src/spark:/opt/spark-apps
    command: >
      bash -c "pip install --upgrade pip &&
      pip install --no-cache-dir -r /requirements.in &&
      airflow webserver"
    ports:
      - "8090:8080"
    restart: always
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 20


  airflow-scheduler:
    image: apache/airflow:latest-python3.12
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    depends_on:
      airflow-webserver:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:
        postgresql+psycopg2://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}/${POSTGRES_DB}
      AIRFLOW__CORE__DEFAULT_TIMEZONE: Asia/Ho_Chi_Minh
      TZ: Asia/Ho_Chi_Minh
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      KAFKA_HOST: ${KAFKA_HOST}
      SCHEMA_REGISTRY_URL: ${SCHEMA_REGISTRY_URL}
      OPENWEATHER_API_URL: ${OPENWEATHER_API_URL}
      OPENWEATHER_API_KEY: ${OPENWEATHER_API_KEY}
      OPENWEATHER_DEST_COORDS: ${OPENWEATHER_DEST_COORDS}
      POSTGRES_HOST: ${POSTGRES_HOST}
      POSTGRES_PORT: ${POSTGRES_PORT}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - ./src/airflow/dags:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/requirements.in:/requirements.in
      - ./src/spark:/opt/spark-apps
    command: >
      bash -c "pip install --no-cache-dir -r /requirements.in &&
      airflow scheduler"
    restart: always
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5


  ### ─── KAFKA ─────────────────────────────────────
  zookeeper:
    image: confluentinc/cp-zookeeper:7.9.0
    container_name: zookeeper
    hostname: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5


  kafka-1:
    image: confluentinc/cp-kafka:7.9.0
    container_name: kafka-1
    hostname: kafka-1
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "19092:19092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:19092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-1:9092,EXTERNAL://localhost:19092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5


  kafka-2:
    image: confluentinc/cp-kafka:7.9.0
    container_name: kafka-2
    hostname: kafka-2
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9093:9092"
      - "19093:19093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:19093
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-2:9092,EXTERNAL://localhost:19093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5


  kafka-3:
    image: confluentinc/cp-kafka:7.9.0
    container_name: kafka-3
    hostname: kafka-3
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9094:9092"
      - "19094:19094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:9092,EXTERNAL://0.0.0.0:19094
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka-3:9092,EXTERNAL://localhost:19094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5


  kafka-init:
    image: confluentinc/cp-kafka:7.9.0
    container_name: kafka-init
    depends_on:
      kafka-1:
        condition: service_started
      kafka-2:
        condition: service_started
      kafka-3:
        condition: service_started
    volumes:
      - ./kafka-init.sh:/init.sh
    entrypoint: [ "/bin/sh", "/init.sh" ]
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics --bootstrap-server kafka-1:9092 --list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5


  schema-registry:
    image: confluentinc/cp-schema-registry:7.9.0
    container_name: schema-registry
    hostname: schema-registry
    depends_on:
      kafka-1:
        condition: service_healthy
    ports:
      - "9020:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: PLAINTEXT://kafka-1:9092,kafka-2:9092,kafka-3:9092
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 10s
      timeout: 5s
      retries: 5


  control-center:
    image: confluentinc/cp-enterprise-control-center:7.9.0
    container_name: control-center
    depends_on:
      kafka-1:
        condition: service_started
      schema-registry:
        condition: service_started
    ports:
      - "9021:9021"
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: kafka-1:9092,kafka-2:9092,kafka-3:9092
      CONTROL_CENTER_ZOOKEEPER_CONNECT: zookeeper:2181
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: http://schema-registry:8081
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      PORT: 9021
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9021"]
      interval: 10s
      timeout: 5s
      retries: 5


  ### ─── PYSPARK (STREAMING CONSUMER) ──────────────
  spark-master:
    image: bitnami/spark:3.5.5-debian-12-r4
    container_name: spark-master
    hostname: spark-master
    environment:
      SPARK_MODE: master
    ports:
      - "7077:7077"
      - "8080:8080"
    volumes:
      - ./spark-shared-data:/shared-data
    command: >
      bash -c "
        spark-class org.apache.spark.deploy.master.Master &
        tail -f /dev/null"
    healthcheck:
      test: ["CMD", "python3", "-c",
             "import http.client; 
             conn = http.client.HTTPConnection('localhost', 8080); 
             conn.request('GET', '/'); 
             res = conn.getresponse(); 
             exit(0) if res.status == 200 else exit(1)"]
      interval: 10s
      timeout: 5s
      retries: 5


  spart_submit:
    build:
      context: .
      dockerfile: Dockerfile-spark-submit
    container_name: spark-submit
    hostname: spark-submit
    depends_on:
      spark-master:
        condition: service_healthy
      kafka-1:
        condition: service_healthy
    volumes:
      - ./src/spark:/opt/spark-apps
      - ./spark-shared-data:/shared-data
    environment:
      KAFKA_HOST: ${KAFKA_HOST}
      KAFKA_TOPIC: ${KAFKA_TOPIC}
      SPARK_MODE: client
      SPARK_MASTER_URL: spark://spark-master:7077
      SCHEMA_REGISTRY_URL: ${SCHEMA_REGISTRY_URL}
      INPUT_PATH: /shared-data/output_raw
      OUTPUT_PATH: /shared-data/output_merged
      TZ: Asia/Ho_Chi_Minh
  

  spark-worker-1:
    image: bitnami/spark:3.5.5-debian-12-r4
    container_name: spark-worker-1
    hostname: spark-worker-1
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-shared-data:/shared-data
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
    ports:
      - "8081:8080"
    healthcheck:
      test: [
        "CMD", "python3", "-c",
        "import socket, sys; \
         s=socket.socket(); \
         s.settimeout(2); \
         sys.exit(0) if s.connect_ex(('spark-master', 8080)) == 0 else sys.exit(1)"
      ]
      interval: 10s
      timeout: 5s
      retries: 5


  spark-worker-2:
    image: bitnami/spark:3.5.5-debian-12-r4
    container_name: spark-worker-2
    hostname: spark-worker-2
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-shared-data:/shared-data
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
    ports:
      - "8082:8080"
    healthcheck:
      test: [
        "CMD", "python3", "-c",
        "import socket, sys; \
         s=socket.socket(); \
         s.settimeout(2); \
         sys.exit(0) if s.connect_ex(('spark-master', 8080)) == 0 else sys.exit(1)"
      ]
      interval: 10s
      timeout: 5s
      retries: 5


  spark-worker-3:
    image: bitnami/spark:3.5.5-debian-12-r4
    container_name: spark-worker-3
    hostname: spark-worker-3
    depends_on:
      spark-master:
        condition: service_healthy
    volumes:
      - ./spark-shared-data:/shared-data
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
    ports:
      - "8083:8080"
    healthcheck:
      test: [
        "CMD", "python3", "-c",
        "import socket, sys; \
         s=socket.socket(); \
         s.settimeout(2); \
         sys.exit(0) if s.connect_ex(('spark-master', 8080)) == 0 else sys.exit(1)"
      ]
      interval: 10s
      timeout: 5s
      retries: 5


volumes:
  postgres-db-volume:
  spark-shared-data:
